---
layout: post  
title: 인공신경망  
description: Deep learning  
---

### 1. 퍼셉트론의 수학적 접근

![사진](/assets/images/deep_learning/2018-10-07/ch03.png)

x1과 x2의 입력값을 받는 퍼셉트론이다. 그림을 수식으로 정리해보면
다음과 같다.
~~~
y = 0 (b + w1 * x1 + w2 * x2 <= 0)
y = 1 (b + w1 * x1 + w2 * x2 > 0)
~~~

여기서 다음과 같이 정리하면

~~~
h(a) = b + w1 * x1 + w2 * x2  이라 하면

y = h(a) 이고

h(a) = 0, 1의 값을 가지게 된다.
~~~

여기서 a = b + w1 * x1 + w2 * x2 가 되고, y는 함수 h(a)에 결과값이 된다.
그리고 a는 함수 h(a)에 입력값이 되면 a 값에 의해서 y의 활성화 정도가 결정되게 된다.
그래서 수식 h(a)의 이름을 ***활성화 함수*** 라고 한다.

### 2. 활성화 함수
퍼셉트론의 결과값(y)는 언제나 0, 1이다. 2가지 값만 존재하기 때문에 중간값에
대한 처리가 불가능하다. 인공신경망에서는 0, 1의 사이의 값이 유연하게 연결되어 있는
시그모이드(sigmoid)함수, 렐루(relu)함수를 대표적인 활성화 함수로 이용하게 된다.


>#### 시그모이드 함수
> 시그모이드 함수를 파이썬으로 구현해 보자
> ~~~
> def sigmoid(x):
>       return 1 / (1 + numpy.exp(-x))
> ~~~
>
>#### 렐루 함수
> 렐루 함수를 파이썬으로 구현해 보자
> ~~~
> def relu(x):
>       return numpy.maximum(0, x)
> ~~~


### 3. 신경망
![신경망](/assets/images/deep_learning/2018-10-12/multiNeuron.png)

사진과 같은 그림의 신경망의 계산을 행렬로 정리하고, 그 의미를 정리해 보자.
> 신경망 1층
> - 입력층으로 데이터의 입력갯수와 동일하다.
>   행렬의 구조 : 1 X 입력값의 갯수  

> 신경망 2층
> - 은닉층으로 임의로 지정하는 뉴런이다.
>   행렬의 구조 : 1층의 입력값의 갯수(이전층의 은닉층의 갯수) X 해당하는 은닉층의 뉴런 갯수

> 신경망 3... n층
> - 은닉층으로 임의로 지정하는 뉴런층이다.
>   행렬의 구조 : 이전층의 은닉층의 갯수 X 해당하는 은닉층의 뉴런 갯수  

> 결과값 층
> - 딥러닝 결과 값
>   행렬의 구조 : 1 X 도출되는 결과값의 갯수
> 
